# coding=utf-8

import requests
import json
import random
import pymysql
from multiprocessing.dummy import Pool as ThreadPool
from bs4 import BeautifulSoup
import sys
import datetime
import time
from imp import reload
import traceback,sys
from requests.exceptions import ProxyError
import re

def LoadUserAgents(uafile):

    uas = []
    with open(uafile, 'rb') as uaf:
        for ua in uaf.readlines():
            if ua:
                uas.append(ua.strip()[1:-1-1])
    random.shuffle(uas)
    return uas

uas = LoadUserAgents("user_agents.txt")
head = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    # 'Referer': 'http://space.bilibili.com/45388',
    # 'Origin': 'http://space.bilibili.com',
    # 'Host': 'space.bilibili.com',
    'AlexaToolbar-ALX_NS_PH': 'AlexaToolbar/alx-4.0',
    'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6,ja;q=0.4',
    'Accept': 'application/json, text/javascript, */*; q=0.01',
}

url="http://www.acfun.cn/v/ac3621153"
response = requests.get(url,headers=head).content

soup=BeautifulSoup(response,"lxml")
pageInfo = soup.find("script",text=re.compile("var pageInfo")).text.replace("var pageInfo =","").replace("false","False").replace("true","True")
pageInfoDict = eval(pageInfo)
contributeTime=pageInfoDict["contributeTime"]
title = pageInfoDict["title"]
description = pageInfoDict["description"]
duration=pageInfoDict["duration"]
banana = pageInfoDict["bananaCount"]
print pageInfoDict
